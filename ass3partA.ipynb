{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd \nfrom keras.utils import np_utils\nimport os\nimport keras\nimport gensim\nfrom random import randint\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom keras.utils import to_categorical\nimport datetime\nfrom numpy import argmax\nfrom keras import Sequential\nfrom keras.layers import LSTM,Input,Lambda,concatenate, Dense,Dropout,GRU,BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text as keras_text, sequence as keras_seq\nimport re\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adadelta\nfrom time import time\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport tensorflow as tf\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split,StratifiedKFold",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29e2950b03297762e4548ac71b866807fc77b5ce"
      },
      "cell_type": "code",
      "source": "def ReadData():\n    train1 = pd.read_csv(\"../input/homedepot/trainbrand.csv\",encoding ='latin1')\n    test1 = pd.read_csv(\"../input/home-depot-product-search-relevance/test.csv\",encoding ='latin1')\n    return train1,test1\ndef ConcatDescAndTitle(df):\n    train1 = df.merge(desc,on='product_uid',how='left')\n    #train1['TitleAndDesc'] = train1[['product_title', 'product_description']].apply(lambda x: ''.join(x), axis=1)\n    train1 = train1.drop(columns=['product_description','id'])\n    #train1['product_title'] = train1['product_title'].str.replace('[^a-zA-Z0-9\\s]','$')\n    #train1['search_term'] = train1['search_term'].str.replace('[^a-zA-Z0-9\\s]','$')\n    train1['product_title'] = train1['product_title'].str.replace('\\\\xa0', '')\n    train1['search_term'] = train1['search_term'].str.replace('\\\\xa0', '')\n    return train1\ndef getBrandAndMatirial():\n    for i in range(test.shape[0]):\n        row = test.loc[i]\n        #foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]\n        row2 = attributes[attributes[\"product_uid\"]==row[\"product_uid\"]]\n        brand = row2[row2[\"name\"]==str(\"MFG Brand Name\")][\"value\"]\n        Material = row2[row2[\"name\"]==str(\"Material\")][\"value\"]\n        if ((brand.values.any())):\n            if ((Material.values.any())):\n                train.loc[i,\"product_title\"] += \" \" + str(brand.values[0]) + \" \" + str(Material.values[0])\n    return train\ndef RemoveNotCommonRelevances():\n    train2 = train\n    train2 = train2[train2.relevance != 1.25]\n    train2 = train2[train2.relevance != 1.5]\n    train2 = train2[train2.relevance != 2.5]\n    train2 = train2[train2.relevance != 1.75]\n    train2 = train2[train2.relevance != 2.5]\n    train2 = train2[train2.relevance != 2.25]\n    train2 = train2[train2.relevance != 2.75]\n    return train2.reset_index(drop=True)\ndef AddMoreIRelevance(train):\n    size = len(chars)\n    for i in range(40000):\n        if (i%5000==0):\n            print(i)\n        title = \"\"\n        term = \"\"\n        for j in range(67):\n            title += chars[randint(0, size-1)]\n            term += chars[randint(0, size-1)]\n        rander = randint(0, 1)\n        rel = 0\n        if (rander==0):\n            rel = 1.0\n        if (rander==1):\n             rel = 1.33\n        train = train.append({'product_title':title,'search_term':term,\"relevance\":rel }, ignore_index=True)\n    return train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "chars = \"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\nlenthofchar = len(chars)\ntrain,test = ReadData()\ntrain = RemoveNotCommonRelevances()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a60bbb758c361613ebbd8557713299cb191bfa1"
      },
      "cell_type": "code",
      "source": "def CreateDic():\n    CharToIndex2 = {}\n    counter=0\n    for key in model.wv.vocab.keys():\n        CharToIndex2[key] = counter\n        counter += 1\n    return CharToIndex2\ndef CreateOneHot(index):\n    arr = np.zeros(lenthofchar)\n    if (index!=-1):\n        arr[index] = 1\n    return arr\ndef CreateCategorial(y):\n    UniquToPred2 = {}\n    PredToUnique2 = {}\n    u = np.unique(y)\n    for i in range(len(u)):\n        UniquToPred2[i] = u[i]\n        PredToUnique2[u[i]] = i\n    return UniquToPred2,PredToUnique2\ndef CreateOneHotLength(index,length):\n    arr = np.zeros(length)\n    arr[index] = 1\n    return arr\ndef CreateY(df):\n    y = np.zeros((df.shape[0],7))\n    for i in range(df.shape[0]):\n            y[i] = CreateOneHotLength(PredToUnique[df[i]],7)\n    return y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a876241138b1aa4d5679b51f13496b6557482707"
      },
      "cell_type": "markdown",
      "source": "****Train Test Split with suffle****"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24074dde91a9b613247e6437eaba33116dfdbf11"
      },
      "cell_type": "code",
      "source": "def spliting(x,trainsize):\n    X_train, X_test = train_test_split(x, test_size=1-trainsize, shuffle=True)\n    X_train = X_train.reset_index()\n    X_train = X_train.drop(columns=['index'])\n    X_test = X_test.reset_index()\n    X_test = X_test.drop(columns=['index'])\n    return X_train, X_test,X_train.relevance,X_test.relevance\ndef LeftRightAndYTrain(df,y,index):\n    left = df[0][index:]\n    right = df[1][index:]\n    y = y[index]\n    y = CreateY(y)\n    return left,right,y\ndef LeftRightAndYTest(df,y,index):\n    left = df[0][index]\n    right = df[1][index]\n    y = y[index]\n    return left,right,y\ndef ReplaceCharsWithOneHotMaxLen(temp):\n    avglen = 0\n    for i in range(temp.shape[0]):\n        lis =(list(temp.loc[i][\"product_title\"]))\n        avglen += len(lis)\n    avglen = int(math.floor(avglen/temp.shape[0]))\n    avglen = 100\n    final_df = np.empty([2,temp.shape[0],avglen,lenthofchar])\n    for i in range(final_df.shape[1]):\n        lis =(list(temp.loc[i][\"product_title\"]))\n        lis2 =(list(temp.loc[i][\"search_term\"]))\n        for j in range(final_df.shape[2]):\n            if (j<len(lis)):\n                final_df[0,i,j]=CreateOneHot(chars.find(lis[j]))\n            else:\n                final_df[0,i,j]= CreateOneHot(-1)\n            if (j<len(lis2)):\n                final_df[1,i,j]= CreateOneHot(chars.find(lis2[j]))\n            else:\n                final_df[1,i,j]=CreateOneHot(-1)\n    return final_df\ndef ConveteBackToPred():\n    newPred = np.zeros(preds.shape[0],)\n    for i in range(preds.shape[0]):\n        maxim=float(0)\n        indx=0\n        for j in range(preds[i].size):\n            if (preds[i][j]>maxim):\n                indx = j\n                maxim = preds[i][j]\n        newPred[i] = UniquToPred[indx] \n    return newPred      ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5a43b67b9dcd8806377cceba6c3385402b02ca4"
      },
      "cell_type": "code",
      "source": "train1, test1,y_train,y_test = spliting(train,0.8)\nUniquToPred,PredToUnique = CreateCategorial(train[\"relevance\"].astype(str))\ny_train = y_train.values\ny_train = y_train.astype(str)\ny_train = CreateY(y_train)\ntrain1 = ReplaceCharsWithOneHotMaxLen(train1)\ntest1 = ReplaceCharsWithOneHotMaxLen(test1)\n#test = ReplaceCharsWithOneHotMaxLen(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88111ba52441464f9a43346d2878e51f589bce5b"
      },
      "cell_type": "markdown",
      "source": "**trying the Kfold**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "679644151c93c5310629c8ffac51179259d183b6"
      },
      "cell_type": "code",
      "source": "def cossim(left, right):\n    ans = (K.sum(left*right))/(K.sqrt(K.sum(K.pow(left,2)))*K.sqrt(K.sum(K.pow(right,2))))\n    ans = tf.nn.relu(ans)\n    return ans*2 +1\ndef custum_mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(((y_pred - y_true)))+(y_pred - y_true)*margin, axis=-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e836b0c2872f8791a1b7954f8400712df673791b"
      },
      "cell_type": "code",
      "source": "def getModel():\n    left_input = Input(shape=(X_train_left_kfold.shape[1], X_train_left_kfold.shape[2]))\n    right_input = Input(shape=(X_train_right_kfold.shape[1], X_train_right_kfold.shape[2]))\n    shared_lstm = LSTM(n_hidden,activation=\"relu\")\n    left_out = shared_lstm(left_input)\n    right_out = shared_lstm(right_input)\n    left_out_norm = BatchNormalization()(left_out)\n    right_out_norm = BatchNormalization()(right_out)\n    merged_vector = concatenate([left_out_norm, right_out_norm], axis=-1)\n    #malstm_distance = Lambda(function=lambda x: cossim(x[0], x[1]),output_shape=lambda x: (1, 1))([left_out, right_out])\n    predictions2 = Dense(20,activation=\"sigmoid\")(merged_vector)\n    predictions4 = Dense(7,activation=\"softmax\")(predictions2)\n    # Pack it all up into a model\n    malstm = Model([left_input, right_input], [predictions4])\n\n    #optimizer = Adadelta(clipnorm=1.25)\n    #malstm.compile(loss='mse', optimizer=\"adam\", metrics=['accuracy'])\n    #sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n\n    malstm.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n    malstm.summary()\n    return malstm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c77f8c54461f8a117aa303bd34e72b32c3e34ab"
      },
      "cell_type": "markdown",
      "source": "**THE LSTM NetWork**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c621041127cb095f45e74bcd4bf08a58e629a4ff",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "n_hidden = 150\nbatch_size = 556\ntotal_rmse = 0\ntotal_mae = 0\nn_epoch = 5\nX_train_left_kfold = train1[0]\nX_train_right_kfold = train1[1]\nX_test_left_kfold = test1[0]\nX_test_right_kfold = test1[1]\nmalstm1 = getModel()\ntraining_start_time = time()\nmalstm_trained = malstm1.fit([X_train_left_kfold,X_train_right_kfold],y_train, batch_size=batch_size, nb_epoch=n_epoch,validation_split=0.05)\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\npreds = malstm1.predict([X_test_left_kfold,X_test_right_kfold], batch_size=batch_size)\npred = ConveteBackToPred()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "addc6a5f8dc5babc7801b85dc1445e5888f6d881"
      },
      "cell_type": "code",
      "source": "rmse = np.sqrt(mean_squared_error(pred, y_test))\ntotal_rmse =+ rmse\nMAE = mean_absolute_error(pred, y_test)\ntotal_mae += MAE\nprint('Test RMSE: %.3f' % rmse)\nprint('Test MAE: %.3f' % MAE)\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\naa=[x for x in range(100)]\nplt.plot(aa, y_test[:100], marker='.', label=\"actual\")\nplt.plot(aa, pred[:100], 'r', label=\"prediction\")\nplt.ylabel('Relevance', size=15)\nplt.xlabel('time', size=15)\nplt.legend(fontsize=15)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "517619de86c8a70fea811522ce8dd537ed7ccec9"
      },
      "cell_type": "code",
      "source": "# Train the model on all the data\nsiamese_model = getModel()\nmalstm_trained = malstm1.fit([X_train_left_kfold,X_train_right_kfold],y_train, batch_size=batch_size, nb_epoch=n_epoch,validation_split=0.05)\n\n# get the output of the concat layer and use it as features to the ml models\nconcat_layer = siamese_model.layers[5].output\nfeature_model = Model(siamese_model.input, concat_layer)\nfeature_model.compile(loss='mse', optimizer='adam')\nprint(feature_model.summary())\n\n# we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\nfeaturs = feature_model.predict([X_train_left_kfold,X_train_right_kfold])\n\n# we preform the prediction also on the test set to evaluate the mse on test set\nfeatures_test = feature_model.predict([X_test_left_kfold,X_test_right_kfold])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5f2b3523fa7d92153f52d5d128be327dc098e81"
      },
      "cell_type": "code",
      "source": "train1, test1,y_train,y_test = spliting(train,0.8)\n#UniquToPred,PredToUnique = CreateCategorial(train[\"relevance\"].astype(str))\ny_train = y_train.values\nfrom xgboost import XGBRegressor\nxgb_model = XGBRegressor(n_estimators=150, learning_rate=0.01, gamma=0, subsample=0.7, colsample_bytree=1, max_depth=6)\nxgb_model.fit(featurs, y_train)\nxgb_pred = xgb_model.predict(features_test)\n\nimport lightgbm as lgb\n# lgb\nlgb_model = lgb.sklearn.LGBMRegressor(is_unbalance=True, learning_rate =0.001, subsample=0.7, colsample_bytree=0.7, max_depth=6)\nlgb_model.fit(featurs, y_train)\nlgb_pred = lgb_model.predict(features_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b5082587515171b397db7ff4cc2089da616d6fb"
      },
      "cell_type": "code",
      "source": "rmse = np.sqrt(mean_squared_error(xgb_pred, y_test))\ntotal_rmse =+ rmse\nMAE = mean_absolute_error(xgb_pred, y_test)\ntotal_mae += MAE\nprint('Test RMSE: %.3f' % rmse)\nprint('Test MAE: %.3f' % MAE)\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\naa=[x for x in range(100)]\nplt.plot(aa, y_test[:100], marker='.', label=\"actual\")\nplt.plot(aa, xgb_pred[:100], 'r', label=\"prediction\")\nplt.ylabel('Relevance', size=15)\nplt.xlabel('time', size=15)\nplt.legend(fontsize=15)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "180f8d4a097a81b096af1dd942fe72e9827237a3"
      },
      "cell_type": "code",
      "source": "xgb_pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c582d8b7ab15c94409dca54f61be188bc86c2dc3"
      },
      "cell_type": "code",
      "source": "lgb_pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1576de2947796f9fd86fe19c757d924887a22a30"
      },
      "cell_type": "code",
      "source": "preds2 = malstm1.predict([test[0],test[1]], batch_size=batch_size)\npred2 = ConveteBackToPred()\nsample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred2\nsample_sub.to_csv('char.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe0016af9964bed4edff4ec1fef715723c4fb28a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}